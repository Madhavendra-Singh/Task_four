# ğŸ§  Task 4: Logistic Regression - Binary Classification

## ğŸ“ Files Included

- `Task4_Intern.ipynb` â€“ Jupyter Notebook with **full code, visualizations, and output**
- `data.csv` â€“ The binary classification dataset (**Breast Cancer Wisconsin Diagnostic Data**)

---

## ğŸ¯ Objective

To build and evaluate a **binary classification model** using **Logistic Regression**, following proper data preprocessing, visualization, model training, evaluation, threshold tuning, and sigmoid explanation.

---

## ğŸ› ï¸ Tools Used

- **Python**  
- **Pandas**, **NumPy** for data handling  
- **Matplotlib**, **Seaborn** for visualization  
- **Scikit-learn** for model building and evaluation  

---

## âœ… Task Overview & Steps Followed

| Step | Description |
|------|-------------|
| 1. ğŸ“¥ **Import Libraries** | Imported all necessary Python libraries including scikit-learn, pandas, and matplotlib |
| 2. ğŸ§¾ **Load Dataset** | Loaded the dataset from `data.csv` |
| 3. ğŸ•µï¸â€â™‚ï¸ **Initial Inspection** | Checked dataset structure, types, and previewed rows |
| 4. ğŸ§¹ **Dropped Unnecessary Columns** | Removed `id` and `Unnamed: 32` columns |
| 5. ğŸ” **Missing Values Check** | Verified that there are no missing values |
| 6. ğŸ“Š **Histograms** | Plotted distributions for all numerical features |
| 7. ğŸ” **Label Encoding** | Encoded the target variable (`diagnosis`: M = 1, B = 0) |
| 8. ğŸ”¥ **Correlation Heatmap** | Visualized feature correlations using a heatmap |
| 9. ğŸ¯ **Feature/Target Split** | Split data into features (X) and target (y) |
| 10. âš–ï¸ **Standardization** | Scaled features using `StandardScaler` |
| 11. ğŸ§ª **Train-Test Split** | Split the data into training and testing sets |
| 12. ğŸ§  **Model Training** | Trained a **Logistic Regression** model |
| 13. ğŸ“ˆ **Model Evaluation** | Evaluated model using:  
    - Confusion Matrix  
    - Accuracy Score  
    - Precision & Recall  
    - ROC AUC Score  
    - Classification Report |
| 14. ğŸ“‰ **ROC Curve** | Plotted the ROC curve and calculated AUC |
| 15. ğŸ§ª **Threshold Tuning** | Adjusted threshold (e.g., 0.3) and observed effect on performance |
| 16. ğŸ§® **Sigmoid Explanation** | Visualized and explained the sigmoid activation function |

---

## ğŸ“Š Model Evaluation Metrics (Threshold = **0.5**)

- **Confusion Matrix:**

```
[[70  1]
 [ 2 41]]
```

- **Accuracy Score**: `0.9737`  
- **Precision Score**: `0.9762`  
- **Recall Score**: `0.9535`  
- **ROC AUC Score**: `0.9697`  

- **Classification Report:**

```
              precision    recall  f1-score   support

           0       0.97      0.99      0.98        71
           1       0.98      0.95      0.96        43

    accuracy                           0.97       114
   macro avg       0.97      0.97      0.97       114
weighted avg       0.97      0.97      0.97       114
```

---

## ğŸ”„ Threshold Tuning (Threshold = **0.3**)

To favor **higher recall** (important in medical diagnosis), the threshold was lowered to `0.3`.

- **Confusion Matrix:**

```
[[67  4]
 [ 1 42]]
```

- **Precision**: `0.9130`  
- **Recall**: `0.9767`  

This demonstrates how adjusting the threshold can improve recall at the cost of a slight drop in precision â€” useful when false negatives are more harmful than false positives.

---

## ğŸ§  Sigmoid Function Explanation

The sigmoid function converts raw output values from the logistic regression model into probabilities between 0 and 1.

### ğŸ”£ Mathematical Formula:

```text
Sigmoid(z) = 1 / (1 + e^(-z))
```

### ğŸ“ LaTeX Format (for documentation or reports):

```latex
\sigma(z) = \frac{1}{1 + e^{-z}}
```

---

## ğŸ”— Author

Internship Task Submission by **Madhavendra Singh**  
Notebook: `Task4_Intern.ipynb`  
Dataset: `data.csv`
